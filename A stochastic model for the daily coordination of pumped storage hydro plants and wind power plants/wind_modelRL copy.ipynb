{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LaTeX for text rendering\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Computer Modern Roman\"],\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"font.size\": 12,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"./Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods - Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given in system definition + the following\n",
    "wind_production_data = [30 for hour in range(1, 25)]\n",
    "demand_data = [50 if hour < 7 or hour > 18 else 100 for hour in range(1, 25)]\n",
    "electricity_sell_price_data = [1 for hour in range(1, 25)]\n",
    "electricity_buy_price_data = [3 for hour in range(1, 25)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour 0: Sell = 30, Buy = 20, Reward = 90\n",
      "Hour 1: Sell = 30, Buy = 20, Reward = 90\n",
      "Hour 2: Sell = 30, Buy = 20, Reward = 90\n",
      "Hour 3: Sell = 30, Buy = 20, Reward = 90\n",
      "Hour 4: Sell = 30, Buy = 20, Reward = 90\n",
      "Hour 5: Sell = 30, Buy = 20, Reward = 90\n",
      "Hour 6: Sell = 30, Buy = 70, Reward = 240\n",
      "Hour 7: Sell = 30, Buy = 70, Reward = 240\n",
      "Hour 8: Sell = 30, Buy = 70, Reward = 240\n",
      "Hour 9: Sell = 30, Buy = 70, Reward = 240\n",
      "Hour 10: Sell = 30, Buy = 70, Reward = 240\n",
      "Hour 11: Sell = 30, Buy = 70, Reward = 240\n",
      "Hour 12: Sell = 30, Buy = 70, Reward = 240\n",
      "Hour 13: Sell = 30, Buy = 70, Reward = 240\n",
      "Hour 14: Sell = 30, Buy = 70, Reward = 240\n",
      "Hour 15: Sell = 30, Buy = 70, Reward = 240\n",
      "Hour 16: Sell = 30, Buy = 70, Reward = 240\n",
      "Hour 17: Sell = 30, Buy = 70, Reward = 240\n",
      "Hour 18: Sell = 30, Buy = 20, Reward = 90\n",
      "Hour 19: Sell = 30, Buy = 20, Reward = 90\n",
      "Hour 20: Sell = 30, Buy = 20, Reward = 90\n",
      "Hour 21: Sell = 30, Buy = 20, Reward = 90\n",
      "Hour 22: Sell = 30, Buy = 20, Reward = 90\n",
      "Hour 23: Sell = 30, Buy = 20, Reward = 90\n"
     ]
    }
   ],
   "source": [
    "states = {\n",
    "    \"wind_production\": wind_production_data,\n",
    "    \"reservoirs_level\": {\n",
    "        \"V1\": [0 for hour in range(1, 25)],\n",
    "        \"V2\": [0 for hour in range(1, 25)],\n",
    "        \"V3\": [0 for hour in range(1, 25)],\n",
    "    }\n",
    "}\n",
    "\n",
    "# actions\n",
    "actions = {\n",
    "    \"sell\": [0 for hour in range(1, 25)],\n",
    "    \"buy\" : [0 for hour in range(1, 25)]\n",
    "}\n",
    "\n",
    "actions[\"sell\"][0] = 2\n",
    "\n",
    "def action(t):\n",
    "    global sell, buy\n",
    "    actions[\"sell\"][t] = min(wind_production_data[t], demand_data[t])\n",
    "    actions[\"buy\"][t] = max(0, demand_data[t] - wind_production_data[t])\n",
    "\n",
    "def reward(t):\n",
    "    return electricity_sell_price_data[t]*actions[\"sell\"][t] + electricity_buy_price_data[t]*actions[\"buy\"][t]\n",
    "\n",
    "for t in range(24):\n",
    "    \n",
    "    action(t)\n",
    "\n",
    "    print(f\"Hour {t}: Sell = {actions['sell'][t]}, Buy = {actions['buy'][t]}, Reward = {reward(t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, filename, reward=None):\n",
    "        if reward is None:\n",
    "            reward = {0: -0.04, 1: 1.0, 2: -1.0, 3: np.NaN}\n",
    "        self.map = np.array(\n",
    "            [[0, 0, 0, 1],\n",
    "            [0, 3, 0, 2],\n",
    "            [0, 0, 0, 0]]\n",
    "        )\n",
    "        self.num_rows = self.map.shape[0]\n",
    "        self.num_cols = self.map.shape[1]\n",
    "        self.num_states = self.num_rows * self.num_cols\n",
    "        self.num_actions = 4\n",
    "        self.reward = reward\n",
    "        self.reward_function = self.get_reward_function()\n",
    "        self.transition_model = self.get_transition_model()\n",
    "\n",
    "    def get_state_from_pos(self, pos):\n",
    "        return pos[0] * self.num_cols + pos[1]\n",
    "\n",
    "    def get_pos_from_state(self, state):\n",
    "        return state // self.num_cols, state % self.num_cols\n",
    "    \n",
    "    def get_reward_function(self):\n",
    "        reward_table = np.zeros(self.num_states)\n",
    "        for r in range(self.num_rows):\n",
    "            for c in range(self.num_cols):\n",
    "                s = self.get_state_from_pos((r, c))\n",
    "                reward_table[s] = self.reward[self.map[r, c]]\n",
    "        return reward_table\n",
    "    \n",
    "    def get_transition_model(self, random_rate=0.2):\n",
    "        transition_model = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
    "        for r in range(self.num_rows):\n",
    "            for c in range(self.num_cols):\n",
    "                s = self.get_state_from_pos((r, c))\n",
    "                neighbor_s = np.zeros(self.num_actions)\n",
    "                if self.map[r, c] == 0:\n",
    "                    for a in range(self.num_actions):\n",
    "                        new_r, new_c = r, c\n",
    "                        if a == 0:\n",
    "                            new_r = max(r - 1, 0)\n",
    "                        elif a == 1:\n",
    "                            new_c = min(c + 1, self.num_cols - 1)\n",
    "                        elif a == 2:\n",
    "                            new_r = min(r + 1, self.num_rows - 1)\n",
    "                        elif a == 3:\n",
    "                            new_c = max(c - 1, 0)\n",
    "                        if self.map[new_r, new_c] == 3:\n",
    "                            new_r, new_c = r, c\n",
    "                        s_prime = self.get_state_from_pos((new_r, new_c))\n",
    "                        neighbor_s[a] = s_prime\n",
    "                else:\n",
    "                    neighbor_s = np.ones(self.num_actions) * s\n",
    "                for a in range(self.num_actions):\n",
    "                    transition_model[s, a, int(neighbor_s[a])] += 1 - random_rate\n",
    "                    transition_model[s, a, int(neighbor_s[(a + 1) % self.num_actions])] += random_rate / 2.0\n",
    "                    transition_model[s, a, int(neighbor_s[(a - 1) % self.num_actions])] += random_rate / 2.0\n",
    "        return transition_model\n",
    "    \n",
    "    def generate_random_policy(self):\n",
    "        return np.random.randint(self.num_actions, size=self.num_states)\n",
    "    \n",
    "    def execute_policy(self, policy, start_pos=(2, 0)):\n",
    "        s = self.get_state_from_pos(start_pos)\n",
    "        total_reward = 0\n",
    "        state_history = [s]\n",
    "        while True:\n",
    "            temp = self.transition_model[s, policy[s]]\n",
    "            s_prime = np.random.choice(self.num_states, p=temp)\n",
    "            state_history.append(s_prime)\n",
    "            r = self.reward_function[s_prime]\n",
    "            total_reward += r\n",
    "            s = s_prime\n",
    "            if r == 1 or r == -1:\n",
    "                break\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mexecute_policy(random_policy)\n\u001b[1;32m      7\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(total_reward)\n\u001b[1;32m      9\u001b[0m hist \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mhist(rewards, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[41], line 79\u001b[0m, in \u001b[0;36mGridWorld.execute_policy\u001b[0;34m(self, policy, start_pos)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition_model[s, policy[s]]\n\u001b[0;32m---> 79\u001b[0m     s_prime \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_states, p\u001b[38;5;241m=\u001b[39mtemp)\n\u001b[1;32m     80\u001b[0m     state_history\u001b[38;5;241m.\u001b[39mappend(s_prime)\n\u001b[1;32m     81\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_function[s_prime]\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:956\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/numpy/core/getlimits.py:484\u001b[0m, in \u001b[0;36mfinfo.__new__\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03mfinfo(dtype)\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m \n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m _finfo_cache \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, dtype):\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_finfo_cache\u001b[38;5;241m.\u001b[39mget(dtype)  \u001b[38;5;66;03m# most common path\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = GridWorld(\"gridworld.txt\")\n",
    "\n",
    "random_policy = env.generate_random_policy()\n",
    "rewards = []\n",
    "for _ in range(1000):\n",
    "    total_reward = env.execute_policy(random_policy)\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "hist = plt.hist(rewards, bins=50, edgecolor=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "class MDP_HydroWind:\n",
    "    def __init__(self):\n",
    "        reward = {0: -0.04, 1: 1.0, 2: -1.0, 3: np.NaN}\n",
    "\n",
    "        self.num_states = self.num_rows * self.num_cols\n",
    "        self.num_actions = 4\n",
    "        self.reward = reward\n",
    "        self.reward_function = self.get_reward_function()\n",
    "        self.transition_model = self.get_transition_model()\n",
    "\n",
    "    def get_state_from_pos(self, pos):\n",
    "        return pos[0] * self.num_cols + pos[1]\n",
    "\n",
    "    def get_pos_from_state(self, state):\n",
    "        return state // self.num_cols, state % self.num_cols\n",
    "    \n",
    "    def get_reward_function(self):\n",
    "        reward_table = np.zeros(self.num_states)\n",
    "        for r in range(self.num_rows):\n",
    "            for c in range(self.num_cols):\n",
    "                s = self.get_state_from_pos((r, c))\n",
    "                reward_table[s] = self.reward[self.map[r, c]]\n",
    "        return reward_table\n",
    "    \n",
    "    def get_transition_model(self, random_rate=0.2):\n",
    "        transition_model = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
    "        for r in range(self.num_rows):\n",
    "            for c in range(self.num_cols):\n",
    "                s = self.get_state_from_pos((r, c))\n",
    "                neighbor_s = np.zeros(self.num_actions)\n",
    "                if self.map[r, c] == 0:\n",
    "                    for a in range(self.num_actions):\n",
    "                        new_r, new_c = r, c\n",
    "                        if a == 0:\n",
    "                            new_r = max(r - 1, 0)\n",
    "                        elif a == 1:\n",
    "                            new_c = min(c + 1, self.num_cols - 1)\n",
    "                        elif a == 2:\n",
    "                            new_r = min(r + 1, self.num_rows - 1)\n",
    "                        elif a == 3:\n",
    "                            new_c = max(c - 1, 0)\n",
    "                        if self.map[new_r, new_c] == 3:\n",
    "                            new_r, new_c = r, c\n",
    "                        s_prime = self.get_state_from_pos((new_r, new_c))\n",
    "                        neighbor_s[a] = s_prime\n",
    "                else:\n",
    "                    neighbor_s = np.ones(self.num_actions) * s\n",
    "                for a in range(self.num_actions):\n",
    "                    transition_model[s, a, int(neighbor_s[a])] += 1 - random_rate\n",
    "                    transition_model[s, a, int(neighbor_s[(a + 1) % self.num_actions])] += random_rate / 2.0\n",
    "                    transition_model[s, a, int(neighbor_s[(a - 1) % self.num_actions])] += random_rate / 2.0\n",
    "        return transition_model\n",
    "    \n",
    "    def generate_random_policy(self):\n",
    "        return np.random.randint(self.num_actions, size=self.num_states)\n",
    "    \n",
    "    def execute_policy(self, policy, start_pos=(2, 0)):\n",
    "        s = self.get_state_from_pos(start_pos)\n",
    "        total_reward = 0\n",
    "        state_history = [s]\n",
    "        while True:\n",
    "            temp = self.transition_model[s, policy[s]]\n",
    "            s_prime = np.random.choice(self.num_states, p=temp)\n",
    "            state_history.append(s_prime)\n",
    "            r = self.reward_function[s_prime]\n",
    "            total_reward += r\n",
    "            s = s_prime\n",
    "            if r == 1 or r == -1:\n",
    "                break\n",
    "        return total_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
